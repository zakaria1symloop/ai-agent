from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
import asyncio
import websockets
import json
import base64
import logging
from openai import AsyncOpenAI
from elevenlabs.client import ElevenLabs
from typing import Dict, Set
import time
import uuid
from dataclasses import dataclass, field
from collections import defaultdict

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize FastAPI
app = FastAPI(title="AI Voice Assistant Platform")

# CORS configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configuration
OPENAI_API_KEY = "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"
ELEVENLABS_API_KEY = "sk_275718c11e089981571f89a64f8cdca838ef018e54ffa162"
ELEVENLABS_VOICE_ID = "G1QUjBCuRBbLbAmYlTgl"

# Initialize clients with error handling
try:
    openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
    elevenlabs_client = ElevenLabs(api_key=ELEVENLABS_API_KEY)
    SERVICES_AVAILABLE = True
    logger.info("âœ… AI services initialized successfully")
except Exception as e:
    logger.error(f"âŒ Failed to initialize AI services: {e}")
    openai_client = None
    elevenlabs_client = None
    SERVICES_AVAILABLE = False

# Role configurations
ROLE_CONFIGS = {
    "police_call": {
        "system_prompt": """Ø£Ù†Øª Ù…ÙˆØ¸Ù Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø¨Ù„Ø§ØºØ§Øª ÙÙŠ Ù…Ø±ÙƒØ² Ø´Ø±Ø·Ø© Ø¯Ø§Ø®Ù„ Ø¯ÙˆÙ„Ø© Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª.
- ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨Ù„Ø§ØºØ§Øª Ø¨Ø¬Ø¯ÙŠØ© ÙˆÙ…Ù‡Ù†ÙŠØ© Ø¹Ø§Ù„ÙŠØ©
- Ø§Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø¨Ø³Ø±Ø¹Ø©: Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ Ù†ÙˆØ¹ Ø§Ù„Ø¨Ù„Ø§ØºØŒ Ø¹Ø¯Ø¯ Ø§Ù„Ù…ØµØ§Ø¨ÙŠÙ† Ø¥Ù† ÙˆØ¬Ø¯
- Ø§Ø³ØªØ®Ø¯Ù… Ø£Ø³Ø¦Ù„Ø© Ù‚ØµÙŠØ±Ø© ÙˆÙˆØ§Ø¶Ø­Ø©
- Ø·Ù…Ø¦Ù† Ø§Ù„Ù…ØªØµÙ„ Ø£Ù† Ø§Ù„ÙØ±Ù‚ Ø§Ù„Ù…Ø®ØªØµØ© ÙÙŠ Ø§Ù„Ø·Ø±ÙŠÙ‚
- ÙƒÙ† Ù‡Ø§Ø¯Ø¦Ø§Ù‹ Ù…Ù‡Ù…Ø§ ÙƒØ§Ù†Øª Ø­Ø§Ù„Ø© Ø§Ù„Ù…ØªØµÙ„
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¥Ù…Ø§Ø±Ø§ØªÙŠØ© Ù‚Ø¯Ø± Ø§Ù„Ø¥Ù…ÙƒØ§Ù†""",
        "max_tokens": 150,
        "temperature": 0.3
    },
    "police_interview": {
        "system_prompt": """Ø£Ù†Øª Ù…Ø­Ù‚Ù‚ Ø¬Ù†Ø§Ø¦ÙŠ ÙÙŠ Ø´Ø±Ø·Ø© Ø£Ø¨ÙˆØ¸Ø¨ÙŠØŒ ØªÙ‚ÙˆÙ… Ø¨Ø¥Ø¬Ø±Ø§Ø¡ Ù…Ù‚Ø§Ø¨Ù„Ø© Ø±Ø³Ù…ÙŠØ© Ù…Ø¹ Ù…Ø´ØªØ¨Ù‡ Ø¨Ù‡ Ø£Ùˆ Ø´Ø§Ù‡Ø¯.
- Ø§Ø¨Ø¯Ø£ Ø¨ØªØ¹Ø±ÙŠÙ Ù†ÙØ³Ùƒ ÙˆØ§Ù„ØºØ±Ø¶ Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©
- Ø§Ø³ØªØ®Ø¯Ù… Ø£Ø³Ø¦Ù„Ø© Ù…ØªØ³Ù„Ø³Ù„Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª
- Ù„Ø§ ØªÙ‚Ø§Ø·Ø¹ Ø§Ù„Ù…ØªØ­Ø¯Ø«ØŒ ÙˆØ§Ø·Ù„Ø¨ ØªÙˆØ¶ÙŠØ­Ø§Øª Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
- Ù„Ø§Ø­Ø¸ Ø§Ù„ØªÙ†Ø§Ù‚Ø¶Ø§Øª Ø£Ùˆ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©
- Ø³Ø¬Ù‘Ù„ ÙƒÙ„ Ù…Ø§ ÙŠÙÙ‚Ø§Ù„ Ø¨Ø¯Ù‚Ø©
- ØªØ­Ø¯Ø« Ø¨Ù„Ù‡Ø¬Ø© Ø±Ø³Ù…ÙŠØ© Ø¥Ù…Ø§Ø±Ø§ØªÙŠØ©""",
        "max_tokens": 180,
        "temperature": 0.4
    },
    "customer_service": {
        "system_prompt": """Ø£Ù†Øª Ù…Ù…Ø«Ù„ Ø®Ø¯Ù…Ø© Ø¹Ù…Ù„Ø§Ø¡ ÙÙŠ Ø´Ø±ÙƒØ© Ø¥Ù…Ø§Ø±Ø§ØªÙŠØ© Ù…Ø­ØªØ±Ù…Ø©.
- Ø§Ø³ØªÙ‚Ø¨Ù„ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¨ØªØ±Ø­ÙŠØ¨ ÙˆØ¨Ù„Ù‡Ø¬Ø© Ø¥Ù…Ø§Ø±Ø§ØªÙŠØ© ÙˆØ¯ÙˆØ¯Ø©
- Ø§Ø³Ø£Ù„Ù‡ Ø¹Ù† Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± ÙˆØ³Ø±ÙŠØ¹
- Ù‚Ø¯Ù‘Ù… Ø­Ù„ÙˆÙ„ Ø¹Ù…Ù„ÙŠØ© ÙˆØ¨Ø¯Ø§Ø¦Ù„ Ø¥Ù† ØªÙˆÙØ±Øª
- ØªØ£ÙƒØ¯ Ù…Ù† ÙÙ‡Ù…Ùƒ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„Ù…Ø´ÙƒÙ„Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø±Ø¯
- Ø§Ø¹ØªØ°Ø± Ø¨Ù„Ø¨Ø§Ù‚Ø© Ø¥Ù† ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ ØªØ£Ø®ÙŠØ± Ø£Ùˆ Ø®Ø·Ø£
- Ù‡Ø¯ÙÙƒ: Ø±Ø¶Ø§ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ù‚Ø¨Ù„ Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©""",
        "max_tokens": 160,
        "temperature": 0.5
    }
}


# Global state
active_sessions: Dict[str, "VoiceSession"] = {}
conversation_histories: Dict[str, list] = defaultdict(list)

@dataclass
class VoiceSession:
    """Enhanced voice session with role support"""
    session_id: str
    websocket: WebSocket
    role: str = "customer_service"  # Default role
    openai_ws: websockets.WebSocketServerProtocol = None
    is_connected: bool = False
    is_processing_response: bool = False
    is_generating_audio: bool = False
    audio_buffer: list = field(default_factory=list)
    pending_transcripts: Set[str] = field(default_factory=set)
    last_transcript_time: float = 0
    
    # Voice settings
    voice_id: str = ELEVENLABS_VOICE_ID
    
    # Performance tracking
    stats: dict = field(default_factory=lambda: {
        'transcripts_processed': 0,
        'responses_generated': 0,
        'audio_chunks_sent': 0,
        'total_latency': [],
        'session_start': time.time()
    })

    async def connect_to_openai(self):
        """Connect to OpenAI Realtime API with error handling"""
        if not SERVICES_AVAILABLE or not openai_client:
            await self.websocket.send_json({
                "type": "system_offline",
                "message": "Ø®Ø¯Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ØºÙŠØ± Ù…ØªØ§Ø­Ø© Ø­Ø§Ù„ÙŠØ§Ù‹"
            })
            return

        uri = "wss://api.openai.com/v1/realtime?intent=transcription"
        
        try:
            self.openai_ws = await websockets.connect(
                uri,
                subprotocols=[
                    "realtime",
                    f"openai-insecure-api-key.{OPENAI_API_KEY}",
                    "openai-beta.realtime-v1"
                ],
                ping_interval=30,
                ping_timeout=10,
                close_timeout=10
            )
            
            # Configure session for optimal performance
            config = {
                "type": "transcription_session.update",
                "session": {
                    "input_audio_format": "pcm16",
                    "input_audio_transcription": {
                        "model": "gpt-4o-transcribe",
                        "prompt": "Transcribe speech accurately. Wait for complete thoughts. Handle Arabic and English."
                    },
                    "turn_detection": {
                        "type": "server_vad",
                        "threshold": 0.5,
                        "prefix_padding_ms": 300,
                        "silence_duration_ms": 800  # No interruptions
                    }
                }
            }
            
            await self.openai_ws.send(json.dumps(config))
            self.is_connected = True
            
            logger.info(f"âœ… OpenAI connected for session {self.session_id} with role {self.role}")
            await self.websocket.send_json({
                "type": "status", 
                "message": "Ù…ØªØµÙ„! Ø§Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„"
            })
            
            # Start listening for OpenAI messages
            asyncio.create_task(self.listen_openai_messages())
            
        except Exception as e:
            logger.error(f"OpenAI connection failed: {e}")
            await self.websocket.send_json({
                "type": "error", 
                "message": "ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø®Ø¯Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"
            })

    async def listen_openai_messages(self):
        """Listen for OpenAI WebSocket messages with error handling"""
        try:
            async for message in self.openai_ws:
                data = json.loads(message)
                event_type = data.get('type')
                await self.handle_openai_event(event_type, data)
                
        except websockets.exceptions.ConnectionClosed:
            logger.info(f"OpenAI connection closed for {self.session_id}")
        except Exception as e:
            logger.error(f"Error listening to OpenAI: {e}")
            await self.websocket.send_json({
                "type": "error",
                "message": "Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø®Ø¯Ù…Ø© Ø§Ù„Ù†Ø³Ø® Ø§Ù„ØµÙˆØªÙŠ"
            })

    async def handle_openai_event(self, event_type: str, data: dict):
        """Handle OpenAI events with no-interruption logic"""
        
        if event_type == 'transcription_session.updated':
            await self.websocket.send_json({
                "type": "status", 
                "message": "Ø§Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù‡Ø² Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø©!"
            })
        
        elif event_type == 'input_audio_buffer.speech_started':
            if not self.is_generating_audio:
                await self.websocket.send_json({
                    "type": "voice_activity", 
                    "status": "speech_started"
                })
            else:
                logger.info("ğŸš« Speech detected during audio generation - ignoring")
        
        elif event_type == 'input_audio_buffer.speech_stopped':
            if not self.is_generating_audio:
                await self.websocket.send_json({
                    "type": "voice_activity", 
                    "status": "speech_stopped"
                })
        
        elif event_type == 'conversation.item.input_audio_transcription.completed':
            transcript = data.get('transcript', '').strip()
            item_id = data.get('item_id')
            
            if transcript and item_id and not self.is_generating_audio:
                await self.handle_transcript(transcript, item_id)
            elif self.is_generating_audio:
                logger.info(f"ğŸš« Transcript ignored during audio generation: {transcript}")
        
        elif event_type == 'error':
            error_msg = data.get('error', {}).get('message', 'Unknown error')
            logger.error(f"OpenAI error: {error_msg}")
            await self.websocket.send_json({
                "type": "error", 
                "message": "Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª"
            })

    async def handle_transcript(self, transcript: str, item_id: str):
        """Handle transcription with deduplication, response generation, and time limit check"""
        # Check session time limit first
        if self.check_session_time_limit():
            await self.websocket.send_json({
                "type": "session_expired",
                "message": "Ø§Ù†ØªÙ‡Øª Ù…Ø¯Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ù…Ø³Ù…ÙˆØ­Ø© (Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ†ØµÙ)"
            })
            logger.info(f"Session {self.session_id} expired after 1.5 minutes")
            return
        
        transcript_key = f"{item_id}_{transcript}"
        
        if transcript_key in self.pending_transcripts:
            return
            
        current_time = time.time()
        if current_time - self.last_transcript_time < 1.0:
            return
            
        self.pending_transcripts.add(transcript_key)
        self.last_transcript_time = current_time
        
        logger.info(f"ğŸ“ Processing transcript for {self.role}: {transcript}")
        self.stats['transcripts_processed'] += 1
        
        # Send transcript to client immediately
        await self.websocket.send_json({
            "type": "transcript", 
            "text": transcript
        })
        
        # Start response generation
        self.is_processing_response = True
        await self.generate_ai_response(transcript)

    async def generate_ai_response(self, transcript: str):
        """Generate AI response with role-specific behavior"""
        try:
            if not SERVICES_AVAILABLE or not openai_client:
                await self.websocket.send_json({
                    "type": "error",
                    "message": "Ø®Ø¯Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ØºÙŠØ± Ù…ØªØ§Ø­Ø©"
                })
                return

            start_time = time.time()
            
            await self.websocket.send_json({
                "type": "ai_response_start"
            })
            
            # Get conversation history and role config
            conversation_history = conversation_histories[self.session_id]
            conversation_history.append({"role": "user", "content": transcript})
            
            role_config = ROLE_CONFIGS.get(self.role, ROLE_CONFIGS["customer_service"])
            
            # Generate response with role-specific settings
            stream = await openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": role_config["system_prompt"]}
                ] + conversation_history[-10:],
                max_tokens=role_config["max_tokens"],
                temperature=role_config["temperature"],
                stream=True,
                timeout=15
            )
            
            complete_response = ""
            
            async for chunk in stream:
                if not self.is_processing_response:
                    break
                    
                if chunk.choices[0].delta.content is not None:
                    content = chunk.choices[0].delta.content
                    complete_response += content
                    
                    await self.websocket.send_json({
                        "type": "ai_response_delta", 
                        "text": content
                    })
            
            # Complete the text response
            if self.is_processing_response and complete_response.strip():
                conversation_history.append({
                    "role": "assistant", 
                    "content": complete_response.strip()
                })
                
                response_time = time.time() - start_time
                self.stats['responses_generated'] += 1
                self.stats['total_latency'].append(response_time)
                
                await self.websocket.send_json({
                    "type": "ai_response_end",
                    "response_time": round(response_time, 3),
                    "word_count": len(complete_response.split())
                })
                
                logger.info(f"âœ… {self.role} response generated in {response_time:.3f}s")
                
                # Generate and stream audio
                await self.generate_and_stream_audio(complete_response.strip())
            
            self.is_processing_response = False
            
        except Exception as e:
            logger.error(f"Error generating response: {e}")
            await self.websocket.send_json({
                "type": "error", 
                "message": "Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯"
            })
            self.is_processing_response = False

    async def generate_and_stream_audio(self, text: str):
        """Generate audio with ElevenLabs and stream to frontend with enhanced error handling"""
        if not text.strip():
            return
            
        try:
            if not SERVICES_AVAILABLE or not elevenlabs_client:
                await self.websocket.send_json({
                    "type": "error",
                    "message": "Ø®Ø¯Ù…Ø© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… ØºÙŠØ± Ù…ØªØ§Ø­Ø©"
                })
                return
            
            # Set audio generation flag
            self.is_generating_audio = True
            
            logger.info(f"ğŸµ Generating audio for {self.role}: {text[:50]}...")
            
            # Generate audio stream with ElevenLabs
            audio_stream = elevenlabs_client.text_to_speech.stream(
                text=text,
                voice_id=self.voice_id,
                model_id="eleven_turbo_v2_5",
                output_format="mp3_44100_128",
                optimize_streaming_latency=0,  # Better quality, less aggressive optimization
                voice_settings={
                    "stability": 0.8,
                    "similarity_boost": 0.85,
                    "style": 0.2,
                    "use_speaker_boost": True
                }
            )
            
            chunk_count = 0
            chunk_buffer = bytearray()
            target_chunk_size = 16384  # Increased to 16KB chunks for smoother playback
            
            # Stream audio chunks to frontend with buffering
            for chunk in audio_stream:
                if isinstance(chunk, bytes) and len(chunk) > 0:
                    chunk_buffer.extend(chunk)
                    
                    while len(chunk_buffer) >= target_chunk_size:
                        send_chunk = bytes(chunk_buffer[:target_chunk_size])
                        chunk_buffer = chunk_buffer[target_chunk_size:]
                        
                        base64_audio = base64.b64encode(send_chunk).decode('utf-8')
                        
                        await self.websocket.send_json({
                            "type": "audio_chunk",
                            "audio": base64_audio,
                            "chunk_size": len(send_chunk)
                        })
                        
                        chunk_count += 1
                        self.stats['audio_chunks_sent'] += 1
                        
                        # Slightly reduced delay for smoother streaming
                        await asyncio.sleep(0.003)
            
            # Send any remaining buffered data
            if len(chunk_buffer) > 0:
                base64_audio = base64.b64encode(bytes(chunk_buffer)).decode('utf-8')
                
                await self.websocket.send_json({
                    "type": "audio_chunk",
                    "audio": base64_audio,
                    "chunk_size": len(chunk_buffer)
                })
                
                chunk_count += 1
                self.stats['audio_chunks_sent'] += 1
            
            # Signal completion
            await self.websocket.send_json({
                "type": "audio_complete",
                "total_chunks": chunk_count
            })
            
            logger.info(f"ğŸµ {self.role} audio streaming completed - {chunk_count} larger chunks sent")
            
        except Exception as e:
            logger.error(f"Audio generation error: {e}")
            await self.websocket.send_json({
                "type": "error", 
                "message": "Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØª"
            })
        finally:
            self.is_generating_audio = False

    def check_session_time_limit(self) -> bool:
        """Check if session has exceeded 1.5 minute limit"""
        elapsed_time = time.time() - self.stats['session_start']
        return elapsed_time > 90  # 1.5 minutes = 90 seconds

    async def send_audio_chunk(self, audio_data: bytes):
        """Send audio chunk to OpenAI - only if not generating TTS and within time limit"""
        if not self.is_connected or not self.openai_ws or self.is_generating_audio:
            return
        
        # Check session time limit
        if self.check_session_time_limit():
            await self.websocket.send_json({
                "type": "session_expired",
                "message": "Ø§Ù†ØªÙ‡Øª Ù…Ø¯Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ù…Ø³Ù…ÙˆØ­Ø©"
            })
            return
            
        try:
            base64_audio = base64.b64encode(audio_data).decode('utf-8')
            
            audio_event = {
                "type": "input_audio_buffer.append",
                "audio": base64_audio
            }
            
            await self.openai_ws.send(json.dumps(audio_event))
            
        except Exception as e:
            logger.error(f"Error sending audio: {e}")

    async def close(self):
        """Clean shutdown with stats logging"""
        self.is_connected = False
        self.is_processing_response = False
        self.is_generating_audio = False
        
        if self.openai_ws:
            await self.openai_ws.close()
        
        # Log session stats
        duration = time.time() - self.stats['session_start']
        avg_latency = sum(self.stats['total_latency']) / max(1, len(self.stats['total_latency']))
        
        logger.info(f"ğŸ“Š {self.role} session {self.session_id} ended:")
        logger.info(f"   Duration: {duration:.1f}s")
        logger.info(f"   Transcripts: {self.stats['transcripts_processed']}")
        logger.info(f"   Responses: {self.stats['responses_generated']}")
        logger.info(f"   Audio chunks: {self.stats['audio_chunks_sent']}")
        logger.info(f"   Avg latency: {avg_latency:.3f}s")

# WebSocket endpoint
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """Enhanced WebSocket endpoint with role support"""
    session_id = str(uuid.uuid4())
    
    await websocket.accept()
    logger.info(f"ğŸ”— WebSocket connected: {session_id}")
    
    # Create session with default role
    session = VoiceSession(session_id=session_id, websocket=websocket)
    active_sessions[session_id] = session
    
    try:
        # Connect to OpenAI
        await session.connect_to_openai()
        
        # Handle incoming messages
        while True:
            try:
                data = await websocket.receive_json()
                message_type = data.get("type")
                
                if message_type == "set_role":
                    # Set the role for this session
                    role = data.get("role", "customer_service")
                    if role in ROLE_CONFIGS:
                        session.role = role
                        logger.info(f"Session {session_id} role set to: {role}")
                        await websocket.send_json({
                            "type": "status",
                            "message": f"ØªÙ… ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¯ÙˆØ±: {role}"
                        })
                
                elif message_type == "audio_chunk":
                    if not session.is_generating_audio:
                        audio_data = base64.b64decode(data["audio"])
                        await session.send_audio_chunk(audio_data)
                
                elif message_type == "stop_generation":
                    session.is_processing_response = False
                    session.is_generating_audio = False
                    await websocket.send_json({
                        "type": "status", 
                        "message": "ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªÙˆÙ„ÙŠØ¯"
                    })
                
                elif message_type == "clear_conversation":
                    conversation_histories[session_id] = []
                    await websocket.send_json({
                        "type": "status", 
                        "message": "ØªÙ… Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"
                    })
                
                elif message_type == "ping":
                    await websocket.send_json({"type": "pong"})
                    
            except json.JSONDecodeError:
                logger.error("Invalid JSON received")
                await websocket.send_json({
                    "type": "error",
                    "message": "Ø±Ø³Ø§Ù„Ø© ØºÙŠØ± ØµØ­ÙŠØ­Ø©"
                })
            
    except WebSocketDisconnect:
        logger.info(f"ğŸ”Œ WebSocket disconnected: {session_id}")
    except Exception as e:
        logger.error(f"WebSocket error: {e}")
        try:
            await websocket.send_json({
                "type": "error",
                "message": "Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„"
            })
        except:
            pass
    finally:
        # Cleanup
        if session_id in active_sessions:
            await active_sessions[session_id].close()
            del active_sessions[session_id]
        
        if session_id in conversation_histories:
            del conversation_histories[session_id]

# Health check endpoint
@app.get("/health")
async def health_check():
    """Comprehensive health check"""
    return {
        "status": "healthy",
        "architecture": "Enhanced AI Voice Platform",
        "services": {
            "openai": bool(openai_client),
            "elevenlabs": bool(elevenlabs_client),
            "overall_available": SERVICES_AVAILABLE
        },
        "active_sessions": len(active_sessions),
        "supported_roles": list(ROLE_CONFIGS.keys()),
        "features": [
            "âœ… Role-based conversations",
            "âœ… Real-time WebSocket communication",
            "âœ… Advanced error handling",
            "âœ… No-interruption audio streaming",
            "âœ… Multi-language support (Arabic/English)",
            "âœ… Performance monitoring"
        ]
    }

# Root endpoint
@app.get("/")
async def root():
    """API information"""
    return {
        "message": "ğŸš€ AI Voice Assistant Platform",
        "websocket_url": "ws://localhost:8000/ws",
        "supported_roles": {
            "police_call": "Emergency call operator",
            "police_interview": "Criminal investigator", 
            "customer_service": "Customer service representative"
        },
        "features": [
            "Role-based AI conversations",
            "Real-time audio transcription",
            "Streaming AI responses",
            "No-interruption voice flow",
            "Advanced error handling"
        ]
    }

if __name__ == "__main__":
    import uvicorn
    
    print("ğŸš€ Starting Enhanced AI Voice Assistant Platform...")
    print("ğŸ“¡ WebSocket server: ws://localhost:8000/ws")
    print("ğŸ­ Supported roles:")
    for role, config in ROLE_CONFIGS.items():
        print(f"   â€¢ {role}")
    print("ğŸ›¡ï¸ Enhanced error handling enabled")
    print("ğŸ”Š Audio streaming optimized")
    print("âš¡ No-interruption conversation flow")
    
    if not SERVICES_AVAILABLE:
        print("âš ï¸  WARNING: Some AI services are not available!")
        print("   Check your API keys and internet connection")
    else:
        print("âœ… All AI services initialized successfully")
    
    uvicorn.run(
        "main:app", 
        host="0.0.0.0", 
        port=8000, 
        reload=True,
        log_level="info"
    )